{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 1.5096187780727632e-05\n",
      "Empirical worst-case gain of the learned model: 1.642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Time parameters\n",
    "T = 10.0\n",
    "dt = 0.01\n",
    "t = np.arange(0, T, dt)\n",
    "\n",
    "# Nonlinear system: m*x'' + c(v)*v + k(x)*x = d(t)\n",
    "def nonlinear_system(t, y, d_func):\n",
    "    x, v = y\n",
    "    d = d_func(t)\n",
    "    m = 1.0\n",
    "    c = 0.5 + 0.1 * v**2  # Nonlinear damping\n",
    "    k = 2.0 + 0.5 * x**2  # Nonlinear spring\n",
    "    dxdt = v\n",
    "    dvdt = (d - c * v - k * x) / m\n",
    "    return [dxdt, dvdt]\n",
    "\n",
    "def generate_trajectory(d_func):\n",
    "    sol = solve_ivp(fun=lambda t, y: nonlinear_system(t, y, d_func),\n",
    "                    t_span=(0, T), y0=[0.0, 0.0], t_eval=t, method='RK45')\n",
    "    return sol.y.T\n",
    "\n",
    "# Generate dataset\n",
    "n_samples = 200\n",
    "X, Y = [], []\n",
    "for _ in range(n_samples):\n",
    "    u = np.random.randn(len(t)) * 0.5\n",
    "    d_func = lambda t_val: np.interp(t_val, t, u)\n",
    "    traj = generate_trajectory(d_func)\n",
    "    X.append(u)\n",
    "    Y.append(traj[:, 0])  # output: position x(t)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Train model\n",
    "model = SimpleNN(len(t))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = criterion(output, Y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final training loss:\", loss.item())\n",
    "\n",
    "# Estimate worst-case gain of learned model\n",
    "def empirical_gain(model, n_trials=500):\n",
    "    max_gain = 0.0\n",
    "    for _ in range(n_trials):\n",
    "        d = np.random.randn(len(t))\n",
    "        d_norm = np.linalg.norm(d)\n",
    "        d_unit = d / (d_norm + 1e-8)\n",
    "        with torch.no_grad():\n",
    "            d_tensor = torch.tensor(d_unit[None, :], dtype=torch.float32)\n",
    "            y = model(d_tensor).numpy().squeeze()\n",
    "        gain = np.linalg.norm(y) / (np.linalg.norm(d_unit) + 1e-8)\n",
    "        max_gain = max(max_gain, gain)\n",
    "    return max_gain\n",
    "\n",
    "gamma_empirical = empirical_gain(model)\n",
    "print(f\"Empirical worst-case gain of the learned model: {gamma_empirical:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
